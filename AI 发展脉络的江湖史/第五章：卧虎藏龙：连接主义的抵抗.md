# 第五章：卧虎藏龙：连接主义的抵抗

在AI的第一个冬天里，虽然大多数研究者都转向了符号主义和专家系统，但仍有一小撮人坚持着神经网络的研究。他们就像是武侠小说中的隐世高手，不问世事，默默修炼自己的内功。

这些人中，有三位特别值得一提：杰弗里·辛顿（Geoffrey Hinton）、约书亚·本吉奥（Yoshua Bengio）和扬·勒坤（Yann LeCun）。他们后来被称为"深度学习三巨头"，对神经网络的复兴起到了决定性的作用。

辛顿出生于英国，是一位数学天才，也是神经网络研究的先驱者之一。早在1970年代，当大多数人都对神经网络失去兴趣时，辛顿仍然坚信这是通向人工智能的正确道路。

"大脑是唯一我们已知的智能系统，"辛顿常说，"如果我们想创造人工智能，模仿大脑是最合理的方法。"

1986年，辛顿和他的同事戴维·鲁梅尔哈特（David Rumelhart）以及罗纳德·威廉姆斯（Ronald Williams）发表了一篇关于"反向传播算法"（Backpropagation）的论文，这成为了神经网络发展史上的一个里程碑。

反向传播算法解决了多层神经网络的训练问题。简单来说，它允许神经网络从错误中学习，通过不断调整网络中的权重，使网络的输出越来越接近预期的结果。这就好比是一位武者通过不断练习，逐渐完善自己的招式，直到达到炉火纯青的境界。

与此同时，本吉奥和勒坤也在各自的研究中推动着神经网络的发展。本吉奥专注于递归神经网络和深度学习理论，而勒坤则开发了卷积神经网络，这是一种特别适合图像识别的神经网络结构。

尽管这些研究者取得了诸多进展，但在当时，神经网络仍然面临着两个主要挑战：计算能力和数据量。

训练一个复杂的神经网络需要大量的计算资源，而20世纪80年代和90年代初的计算机性能还远远不够。这就好比是一位武者想要修炼高深的内功，但体力和精力都有限，难以达到理想的效果。

此外，神经网络的训练也需要大量的数据样本。没有足够的训练数据，网络就难以学到有用的模式和特征。而在那个年代，大规模的数据集还很少见。

尽管如此，这些早期的研究者仍然坚持自己的信念，在逆境中前行。他们的坚持，为后来神经网络的复兴奠定了基础。

到了1990年代，随着计算机硬件性能的提升和互联网的普及，神经网络研究开始有了新的机会。

1989年，勒坤在贝尔实验室开发出了一种基于卷积神经网络的手写数字识别系统，称为LeNet。这个系统能够以99.3%的准确率识别手写数字，被美国邮政服务用于自动分拣邮件。

1997年，德国人于尔根·施密德胡贝尔（Jürgen Schmidhuber）和他的学生塞普·霍克雷特（Sepp Hochreiter）提出了长短期记忆网络（LSTM），这是一种能够学习长序列依赖关系的递归神经网络，对于处理时间序列数据（如语音和文本）非常有效。

2006年，辛顿和他的团队发表了一篇关于"深度信念网络"（Deep Belief Networks）的论文，展示了如何有效地训练深层神经网络。这被认为是深度学习复兴的开始。

"深度学习的核心思想很简单，"辛顿解释道，"就是通过多层神经网络，让计算机自动学习数据中的层次特征，从简单到复杂，从具体到抽象。"

这种方法特别适合处理非结构化数据，如图像、声音和自然语言，这恰恰是传统AI方法的弱项。

与此同时，计算机硬件的发展也为深度学习提供了强大的支持。特别是图形处理单元（GPU）的出现，极大地加速了神经网络的训练过程。

GPU原本是为了加速图形渲染而设计的，但研究者们发现，它的并行处理能力也非常适合神经网络的矩阵运算。使用GPU进行神经网络训练，速度可以比传统CPU快几十倍甚至上百倍。

"这就像是武者突然获得了一种神奇的丹药，能够在短时间内大幅提升内力，"一位研究者这样形容GPU对神经网络研究的影响。

2009年，斯坦福大学的李飞飞（Fei-Fei Li）和她的团队发布了ImageNet数据集，这是一个包含超过1400万张标注图像的大规模数据集。ImageNet的出现，为深度学习的发展提供了宝贵的训练资源。

"在人工智能的发展历程中，数据就像是阳光和雨露，算法就像是种子，计算力就像是土壤，"李飞飞这样描述数据的重要性，"没有足够的阳光和雨露，再好的种子也难以茁壮成长。"

有了ImageNet这样的大规模数据集，研究者们终于有了足够的"阳光和雨露"来培育他们的神经网络"种子"。

2010年，李飞飞团队又发起了ImageNet大规模视觉识别挑战赛（ILSVRC），这是一个基于ImageNet数据集的年度计算机视觉竞赛。这个比赛就像是AI江湖中的"华山论剑"，吸引了世界各地的研究者参与，推动了计算机视觉技术的快速发展。

就这样，经过多年的潜心研究和积累，神经网络研究者们终于迎来了属于自己的春天。一场深度学习的革命，正在悄然酝酿……
